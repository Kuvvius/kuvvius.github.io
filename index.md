
---
layout: custom_homepage
title: About Me -Jiawei Gu
permalink: /

# 其他你可能需要的 Front Matter 变量
# title: "Your Custom Home Page Title" # 如果你希望主页标题与 site.title 不同
# description: "Your custom description for this page"
---


<section id="about" class="content-section">
    <h2 class="section-title">About Me</h2>
    <div class="about-content">
        <p>I am Jiawei Gu, an MSc student in Computer Science at Sun Yat-sen University, expected to graduate in June 2025. My research journey started later than usual due to my volunteer service experience. However, I possess an unparalleled passion for exploration and scientific inquiry.</p>

        <p>Over the past year of dedicated research study, I have solidified my aspiration to contribute to the field of Natural Language Processing (NLP). I am actively pursuing this goal, much like my unwavering commitment to morning runs over the past five years. I firmly believe that "pain is inevitable, suffering is optional" and I am ready to embrace the challenges that come with pursuing what I desire.</p>

        <p>Specifically, I have delved into research on natural language and multi-modal approaches, shaping me into an independent researcher with clear ideas about future research directions.</p>

        <h3 style="margin-top: 30px; margin-bottom: 15px; color: #2c3e50;">Research Interests</h3>
        <ul class="interests-list">
            <li>Scaling of Text/Multimodal Reasoning</li>
            <li>Efficient Learning for Training and Inference</li>
            <li>Domain Knowledge Injection</li>
        </ul>
    </div>
</section>

<section id="education" class="content-section">
    <h2 class="section-title">Education</h2>
    <div class="education-item">
        <h3>Master of Science in Computer Science</h3>
        <p><strong>Sun Yat-sen University</strong> • Expected June 2025</p>
    </div>
    <div class="education-item">
        <h3>Bachelor of Information Management and Information System</h3>
        <p><strong>Sun Yat-sen University</strong> • September 2017 to June 2021</p>
    </div>
</section>

<section id="experience" class="content-section">
    <h2 class="section-title">Experience</h2>
    <div class="experience-item">
        <h3>Shanghai AILab</h3>
        <p><strong>NLP Research Intern</strong> • October 2024 - Present</p>
        <p>Supervisor: Prof. Yu Cheng • Shanghai, China</p>
    </div>
    <div class="experience-item">
        <h3>Tencent</h3>
        <p><strong>NLP Research Intern</strong> • May 2024 - September 2024</p>
        <p>Shenzhen, China</p>
    </div>
    <div class="experience-item">
        <h3>International Digital Economy Academy (IDEA) Lab</h3>
        <p><strong>NLP Research Intern</strong> • May 2023 - October 2024</p>
        <p>Remote</p>
    </div>
    <div class="experience-item">
        <h3>SenseTime Research Institute</h3>
        <p><strong>NLP Research Intern</strong> • November 2023 - April 2024</p>
        <p>Shanghai</p>
    </div>
</section>

<section id="news" class="content-section">
    <h2 class="section-title">News</h2>
    <div class="news-item">
        <span class="news-date">July 2025:</span> My paper "EMMA: An Enhanced MultiModal Reasoning Benchmark" has been accepted to ICML 2025 as an Oral presentation!
    </div>
    <div class="news-item">
        <span class="news-date">July 2025:</span> My paper "Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively" has been accepted to ACL 2025 as an Oral Presentation!
    </div>
    <div class="news-item">
        <span class="news-date">July 2025:</span> My paper "Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience" has been accepted to ACL 2025!
    </div>
    <div class="news-item">
        <span class="news-date">November 2024:</span> My paper "CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models" has been accepted to EMNLP 2024 as a First Author paper and selected for Oral Presentation!
    </div>
</section>

<section id="publications" class="content-section">
    <h2 class="section-title">Publications</h2>

    <div class="publication-item">
        <div class="publication-title">CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models</div>
        <div class="publication-authors"><strong>Jiawei Gu</strong>, et al.</div>
        <div class="publication-venue">EMNLP 2024, First Author, Oral Presentation</div>
        <div class="publication-links">
            <a href="{{ 'YOUR_PDF_LINK_FOR_CMR' | relative_url }}" target="_blank">PDF</a>
            <a href="{{ 'YOUR_CODE_LINK_FOR_CMR' | relative_url }}" target="_blank">Code</a>
            <a href="{{ 'YOUR_SLIDES_LINK_FOR_CMR' | relative_url }}" target="_blank">Slides</a>
        </div>
    </div>

    <div class="publication-item">
        <div class="publication-title">EMMA: An Enhanced MultiModal Reasoning Benchmark</div>
        <div class="publication-authors"><strong>Jiawei Gu</strong>, et al.</div>
        <div class="publication-venue">ICML 2025 (Oral), First Author</div>
        <div class="publication-links">
            <a href="{{ '/data/JiaweiGu_ICML.pdf' | relative_url }}" target="_blank">PDF</a>
            <a href="{{ 'YOUR_HOMEPAGE_LINK_FOR_EMMA' | relative_url }}" target="_blank">Homepage</a>
            <a href="{{ 'YOUR_SLIDES_LINK_FOR_EMMA' | relative_url }}" target="_blank">Slides</a>
        </div>
    </div>

    <div class="publication-item">
        <div class="publication-title">Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively</div>
        <div class="publication-authors"><strong>Jiawei Gu</strong>, et al.</div>
        <div class="publication-venue">ACL 2025 (Oral Presentation), First Author</div>
        <div class="publication-links">
            <a href="{{ 'YOUR_PDF_LINK_FOR_BOOSTING' | relative_url }}" target="_blank">PDF</a>
        </div>
    </div>

    <div class="publication-item">
        <div class="publication-title">Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience</div>
        <div class="publication-authors"><strong>Jiawei Gu</strong>, et al.</div>
        <div class="publication-venue">ACL 2025, First Author</div>
        <div class="publication-links">
            <a href="{{ 'YOUR_PDF_LINK_FOR_STRUCTURED_KNOWLEDGE' | relative_url }}" target="_blank">PDF</a>
        </div>
    </div>
</section>
<!-- 
# About Me

[cite_start]I am Jiawei Gu, an MSc student in Computer Science at Sun Yat-sen University, expected to graduate in June 2025[cite: 1]. 

[cite_start]My research journey started later than usual due to my volunteer service experience[cite: 20]. [cite_start]However, I possess an unparalleled passion for exploration and scientific inquiry[cite: 21]. [cite_start]Over the past year of dedicated research study, I have solidified my aspiration to contribute to the field of Natural Language Processing (NLP)[cite: 22]. [cite_start]I am actively pursuing this goal, much like my unwavering commitment to morning runs over the past five years[cite: 22]. [cite_start]I firmly believe that "pain is inevitable, suffering is optional" and I am ready to embrace the challenges that come with pursuing what I desire[cite: 23]. [cite_start]Specifically, I have delved into research on natural language and multi-modal approaches, shaping me into an independent researcher with clear ideas about future research directions[cite: 24].

My research interests include:
* [cite_start]Scaling of Text/Multimodal Reasoning [cite: 24]
* [cite_start]Efficient Learning for Training and Inference [cite: 24]
* [cite_start]Domain Knowledge Injection [cite: 24]

---

# Education

* **Sun Yat-sen University**
    * [cite_start]Master of Science in Computer Science. [cite: 2]


* **Sun Yat-sen University**
    * [cite_start]Bachelor of Information Management and Information System. [cite: 2]

---

# Experience

* [cite_start]**Shanghai AILab** [cite: 3]
    * NLP Research Intern. [cite_start]Supervisor: Prof. Yu Cheng [cite: 3]
    * [cite_start]October 2024 - Present [cite: 3]
    * [cite_start]Shanghai, China [cite: 3]

* [cite_start]**Tencent** [cite: 3]
    * [cite_start]NLP Research Intern. [cite: 3]
    * [cite_start]May 2024 - September 2024 [cite: 3]
    * [cite_start]Shenzhen, China [cite: 3]

* [cite_start]**International Digital Economy Academy (IDEA) Lab** [cite: 3]
    * [cite_start]NLP Research Intern. [cite: 3]
    * [cite_start]May 2023 - October 2024 [cite: 3]
    * [cite_start]Remote [cite: 3]

* [cite_start]**SenseTime Research Institute** [cite: 3]
    * [cite_start]NLP Research Intern. [cite: 3]
    * [cite_start]November 2023 - April 2024 [cite: 3]
    * [cite_start]Shanghai [cite: 3]

---

# News

* [cite_start]**July 2025:** My paper, "[EMMA: An Enhanced MultiModal Reasoning Benchmark](YOUR_EMMA_ARXIV_LINK)"[cite: 6], has been accepted to ICML 2025 as an Oral presentation!
* [cite_start]**July 2025:** My paper, "[Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively](YOUR_SRM_ARXIV_LINK)"[cite: 9], has been accepted to ACL 2025 as an Oral Presentation!
* [cite_start]**July 2025:** My paper, "[Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](YOUR_CORE_ARXIV_LINK)"[cite: 11], has been accepted to ACL 2025!
* [cite_start]**July 2025:** My paper, "[MolRAG: Unlocking the Power of Large Language Models for Molecular Property Prediction](YOUR_MOLRAG_ARXIV_LINK)"[cite: 15], has been accepted to ACL 2025!
* [cite_start]**November 2024:** My paper, "[CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models](YOUR_CMR_ARXIV_LINK)"[cite: 4], has been accepted to EMNLP 2024 as a First Author paper and selected for Oral Presentation!

---

# Publications

1.  [cite_start]**[CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models](YOUR_PAPER_PDF_LINK)** [cite: 4]
    **Jiawei Gu***, et al.
    [cite_start]EMNLP 2024, **First Author**, Oral Presentation. [cite: 4]
    * [cite_start]**Contribution**: We attempt to re-visit the scaling behavior of LLMs under the hood of CPT, and introduce Critical Mixture Ratio (CMR) and a CMR scaling law, providing a predictive framework and practical guidelines for optimizing LLM training in specialized domains, ensuring both general and domain-specific performance while managing training resources effectively[cite: 5].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Slides](YOUR_SLIDES_LINK)

2.  **[Can MLLMs Reason/Think in Multimodality? [cite_start]EMMA: An Enhanced MultiModal Reasoning Benchmark](YOUR_PAPER_PDF_LINK)** [cite: 6]
    **Jiawei Gu***, et al.
    [cite_start]ICML 2025 (**Oral**), **First Author**. [cite: 6]
    * [cite_start]**Contribution**: This work introduces EMMA, a benchmark designed to evaluate multimodal reasoning across mathematics, physics, chemistry, and coding[cite: 7]. [cite_start]EMMA features tasks requiring advanced visual manipulation and cross-modal reasoning, providing insights into the limitations of current MLLMs and emphasizing the need for improved architectures and training paradigms[cite: 8].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Homepage](YOUR_PAPER_HOMEPAGE_LINK) &nbsp; [Slides](YOUR_SLIDES_LINK)

3.  [cite_start]**[Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively](YOUR_PAPER_PDF_LINK)** [cite: 9]
    **Jiawei Gu***, et al.
    [cite_start]ACL 2025 (**Oral Presentation**), **First Author**. [cite: 9]
    * [cite_start]**Contribution**: A Plug and Play framework with Speculative Reward Models (SRM), which simplifies the complex process of achieving an optimal balance between effectiveness and efficiency[cite: 10].
    [PDF](YOUR_PAPER_PDF_LINK)

4.  [cite_start]**[Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](YOUR_PAPER_PDF_LINK)** [cite: 11]
    **Jiawei Gu***, et al.
    [cite_start]ACL 2025, **First Author**. [cite: 11]
    * [cite_start]**Contribution**: A plug-and-play method, CoRE, is proposed to improve the reasoning ability on structured knowledge[cite: 12]. [cite_start]It is training-free, lifelong, and continuous[cite: 13].
    [PDF](YOUR_PAPER_PDF_LINK)

5.  [cite_start]**[A Survey on LLM-as-a-Judge](YOUR_PAPER_PDF_LINK)** [cite: 13]
    **Jiawei Gu***, et al.
    [cite_start]Preprint, **First Author**. [cite: 13]
    * [cite_start]**Contribution**: This survey provides a comprehensive review of strategies to enhance the reliability of LLM-as-a-Judge systems, introduces a benchmark for systematic evaluation, and discusses practical applications, challenges, and future directions to guide research and deployment[cite: 14].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Homepage](YOUR_PAPER_HOMEPAGE_LINK)

6.  [cite_start]**[MolRAG: Unlocking the Power of Large Language Models for Molecular Property Prediction](YOUR_PAPER_PDF_LINK)** [cite: 15]
    Second Author.
    [cite_start]ACL 2025. [cite: 15]

7.  [cite_start]**[Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](YOUR_PAPER_PDF_LINK)** [cite: 16]
    Second Author.
    [cite_start]Preprint. [cite: 16]

8.  [cite_start]**[Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations](YOUR_PAPER_PDF_LINK)** [cite: 17]
    Third Author.
    [cite_start]Preprint. [cite: 17]
    [PDF](YOUR_PAPER_PDF_LINK)

9.  [cite_start]**[Full Front: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](YOUR_PAPER_PDF_LINK)** [cite: 16]
    Third Author.
    [cite_start]Preprint. [cite: 16]
    [PDF](YOUR_PAPER_PDF_LINK)

---
 -->
