---
layout: page
title: About Me â€“ Jiawei Gu
permalink: /
---

# About Me

[cite_start]I am Jiawei Gu, an MSc student in Computer Science at Sun Yat-sen University, expected to graduate in June 2025[cite: 1]. 

[cite_start]My research journey started later than usual due to my volunteer service experience[cite: 20]. [cite_start]However, I possess an unparalleled passion for exploration and scientific inquiry[cite: 21]. [cite_start]Over the past year of dedicated research study, I have solidified my aspiration to contribute to the field of Natural Language Processing (NLP)[cite: 22]. [cite_start]I am actively pursuing this goal, much like my unwavering commitment to morning runs over the past five years[cite: 22]. [cite_start]I firmly believe that "pain is inevitable, suffering is optional" and I am ready to embrace the challenges that come with pursuing what I desire[cite: 23]. [cite_start]Specifically, I have delved into research on natural language and multi-modal approaches, shaping me into an independent researcher with clear ideas about future research directions[cite: 24].

My research interests include:
* [cite_start]Scaling of Text/Multimodal Reasoning [cite: 24]
* [cite_start]Efficient Learning for Training and Inference [cite: 24]
* [cite_start]Domain Knowledge Injection [cite: 24]

---


# News

* [cite_start]**July 2025:** My paper, "[EMMA: An Enhanced MultiModal Reasoning Benchmark](YOUR_EMMA_ARXIV_LINK)"[cite: 6], has been accepted to ICML 2025 as an Oral presentation!
* [cite_start]**July 2025:** My paper, "[Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively](YOUR_SRM_ARXIV_LINK)"[cite: 9], has been accepted to ACL 2025 as an Oral Presentation!
* [cite_start]**July 2025:** My paper, "[Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](YOUR_CORE_ARXIV_LINK)"[cite: 11], has been accepted to ACL 2025!
* [cite_start]**July 2025:** My paper, "[MolRAG: Unlocking the Power of Large Language Models for Molecular Property Prediction](YOUR_MOLRAG_ARXIV_LINK)"[cite: 15], has been accepted to ACL 2025!
* [cite_start]**November 2024:** My paper, "[CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models](YOUR_CMR_ARXIV_LINK)"[cite: 4], has been accepted to EMNLP 2024 as a First Author paper and selected for Oral Presentation!

---

# Publications

1.  [cite_start]**[CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models](YOUR_PAPER_PDF_LINK)** [cite: 4]
    **Jiawei Gu***, et al.
    [cite_start]EMNLP 2024, **First Author**, Oral Presentation. [cite: 4]
    * [cite_start]**Contribution**: We attempt to re-visit the scaling behavior of LLMs under the hood of CPT, and introduce Critical Mixture Ratio (CMR) and a CMR scaling law, providing a predictive framework and practical guidelines for optimizing LLM training in specialized domains, ensuring both general and domain-specific performance while managing training resources effectively[cite: 5].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Slides](YOUR_SLIDES_LINK)

2.  **[Can MLLMs Reason/Think in Multimodality? [cite_start]EMMA: An Enhanced MultiModal Reasoning Benchmark](YOUR_PAPER_PDF_LINK)** [cite: 6]
    **Jiawei Gu***, et al.
    [cite_start]ICML 2025 (**Oral**), **First Author**. [cite: 6]
    * [cite_start]**Contribution**: This work introduces EMMA, a benchmark designed to evaluate multimodal reasoning across mathematics, physics, chemistry, and coding[cite: 7]. [cite_start]EMMA features tasks requiring advanced visual manipulation and cross-modal reasoning, providing insights into the limitations of current MLLMs and emphasizing the need for improved architectures and training paradigms[cite: 8].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Homepage](YOUR_PAPER_HOMEPAGE_LINK) &nbsp; [Slides](YOUR_SLIDES_LINK)

3.  [cite_start]**[Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively](YOUR_PAPER_PDF_LINK)** [cite: 9]
    **Jiawei Gu***, et al.
    [cite_start]ACL 2025 (**Oral Presentation**), **First Author**. [cite: 9]
    * [cite_start]**Contribution**: A Plug and Play framework with Speculative Reward Models (SRM), which simplifies the complex process of achieving an optimal balance between effectiveness and efficiency[cite: 10].
    [PDF](YOUR_PAPER_PDF_LINK)

4.  [cite_start]**[Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](YOUR_PAPER_PDF_LINK)** [cite: 11]
    **Jiawei Gu***, et al.
    [cite_start]ACL 2025, **First Author**. [cite: 11]
    * [cite_start]**Contribution**: A plug-and-play method, CoRE, is proposed to improve the reasoning ability on structured knowledge[cite: 12]. [cite_start]It is training-free, lifelong, and continuous[cite: 13].
    [PDF](YOUR_PAPER_PDF_LINK)

5.  [cite_start]**[A Survey on LLM-as-a-Judge](YOUR_PAPER_PDF_LINK)** [cite: 13]
    **Jiawei Gu***, et al.
    [cite_start]Preprint, **First Author**. [cite: 13]
    * [cite_start]**Contribution**: This survey provides a comprehensive review of strategies to enhance the reliability of LLM-as-a-Judge systems, introduces a benchmark for systematic evaluation, and discusses practical applications, challenges, and future directions to guide research and deployment[cite: 14].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Homepage](YOUR_PAPER_HOMEPAGE_LINK)

6.  [cite_start]**[MolRAG: Unlocking the Power of Large Language Models for Molecular Property Prediction](YOUR_PAPER_PDF_LINK)** [cite: 15]
    Second Author.
    [cite_start]ACL 2025. [cite: 15]

7.  [cite_start]**[Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](YOUR_PAPER_PDF_LINK)** [cite: 16]
    Second Author.
    [cite_start]Preprint. [cite: 16]

8.  [cite_start]**[Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations](YOUR_PAPER_PDF_LINK)** [cite: 17]
    Third Author.
    [cite_start]Preprint. [cite: 17]
    [PDF](YOUR_PAPER_PDF_LINK)

9.  [cite_start]**[Full Front: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](YOUR_PAPER_PDF_LINK)** [cite: 16]
    Third Author.
    [cite_start]Preprint. [cite: 16]
    [PDF](YOUR_PAPER_PDF_LINK)

---

