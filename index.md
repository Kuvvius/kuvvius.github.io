---
layout: page
title: About Me â€“ Jiawei Gu
permalink: /
---

---
layout: custom_homepage
permalink: /
---

### About Me

I am now a first-year PhD candidate at [National University of Singapore](https://nus.edu.sg/) (NUS), supervised by Dr. [Michael Qizhe Shieh](https://michaelshieh.com/).
My research is driven by the passion for advancing the edge intelligence of large language models (LLMs). I am particularly focused on the paradigm shift [from deep to long learning](https://hazyresearch.stanford.edu/blog/2023-03-27-long-learning), aiming to enhance LLM capabilities by learning effectively from substantial context. My recent work concentrates on improving LLMs' in-depth understanding of complex context, specifically targeting:

- Effective and efficient long-context modeling;
- Unified understanding of multimodal context.

---

### News
- ***May 2025:*** [RAPID](https://arxiv.org/abs/2502.20330) has been accepted to ICML 2025 as Spotlight!
- ***Feb 2025:*** We release the [LongPO](https://www.arxiv.org/pdf/2502.13922), a self-evolving long-context LLM training approach for both context extension and long-context alignment in one stage without external annotation.
- ***Feb 2025:*** [LongPO](https://www.arxiv.org/pdf/2502.13922) has been accepted to ICLR 2025!
- ***Jan 2024:*** [CLEX](https://arxiv.org/abs/2310.16450) has been accepted to ICLR 2024!
- ***Oct 2023:*** We release the [CLEX](https://arxiv.org/abs/2310.16450), a length extrapolation method that enables LLMs to access the context length up to 4x~8x the training length!

---

### Publications

- [RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding](https://arxiv.org/abs/2502.20330)<br>
  **Guanzheng Chen**\*, Qilong Feng\*, Jinjie Ni, Xin Li, Michael Qizhe Shieh.<br>
   The Forty-Second International Conference on Machine Learning ([ICML'25](https://icml.cc/), ***Spotlight***)

  <div class="btn-links">
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2502.20330" target="_blank" rel="noopener">PDF</a>
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/John-AI-Lab/RAPID" target="_blank" rel="noopener">Code</a>
  </div>

- [LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization](https://arxiv.org/abs/2502.13922)<br>
  **Guanzheng Chen**, Xin Li, Michael Qizhe Shieh, Lidong Bing.<br>
 The Thirteenth International Conference on Learning Representations ([ICLR'25](https://iclr.cc/))
  
  <div class="btn-links">
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://www.arxiv.org/pdf/2502.13922" target="_blank" rel="noopener">PDF</a>
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/DAMO-NLP-SG/LongPO" target="_blank" rel="noopener">Code</a>
  </div>


- [VCD: Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding](https://arxiv.org/abs/2311.16922)<br>
 Sicong Leng, Hang Zhang, **Guanzheng Chen**, Xin Li, Shijian Lu, Chunyan Miao, Lidong Bing.<br>
 The IEEE/CVF Conference on Computer Vision and Pattern Recognition 2024 ([CVPR'24](https://cvpr.thecvf.com/Conferences/2024))
  
  <div class="btn-links">
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2311.16922" target="_blank" rel="noopener">PDF</a>
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/DAMO-NLP-SG/VCD" target="_blank" rel="noopener">Code</a>
  </div>



- [CLEX: Continuous Length Extrapolation for Large Language Models](https://arxiv.org/pdf/2310.16450.pdf)<br>
  **Guanzheng Chen**, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing.<br>
 The Twelfth International Conference on Learning Representations ([ICLR'24](https://iclr.cc/))
  
  <div class="btn-links">
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2310.16450.pdf" target="_blank" rel="noopener">PDF</a>
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/DAMO-NLP-SG/CLEX" target="_blank" rel="noopener">Code</a>
  </div>


- [Revisiting Parameter-Efficient Tuning: Are We Really There Yet?](https://arxiv.org/abs/2202.07962)<br>
  **Guanzheng Chen**, Fangyu Liu, Zaiqiao Meng, Shangsong Liang.<br>
  The 2022 Conference on Empirical Methods in Natural Language Processing ([EMNLP'22](https://2022.emnlp.org/), ***Oral Presentation***).
  
  <div class="btn-links">
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/pdf/2202.07962.pdf" target="_blank" rel="noopener">PDF</a>
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/guanzhchen/petuning" target="_blank" rel="noopener">Code</a>
  </div>

- [Multi-Relational Graph Representation Learning with Bayesian Gaussian Process Network](https://ojs.aaai.org/index.php/AAAI/article/view/20492)<br>
  **Guanzheng Chen**, Jinyuan Fang, Zaiqiao Meng, Qiang Zhang, Shangsong Liang.<br>
  Thirty-Sixth AAAI Conferene on Artificial Intelligence ([AAAI'22](https://aaai.org/Conferences/AAAI-22/)).<br>
  
  <div class="btn-links">
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="{{ '/data/papers/8491.ChenG_with_appendix.pdf' | relative_url }}" target="_blank" rel="noopener">PDF</a>
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/sysu-gzchen/GGPN" target="_blank" rel="noopener">Code</a>
  </div>

---

### Services

- Conference reviewer: [Neurips 2024](https://neurips.cc/Conferences/2024), [ICLR 2025](https://iclr.cc/), [ICML 2025](https://icml.cc/), [ACL Roling Review ](https://aclrollingreview.org/)

- Journal reviewer: [Neurocomputing](https://www.sciencedirect.com/journal/neurocomputing), [IEEE Transactions on Pattern Analysis and Machine Intelligence](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34)
<!-- 
# About Me

[cite_start]I am Jiawei Gu, an MSc student in Computer Science at Sun Yat-sen University, expected to graduate in June 2025[cite: 1]. 

[cite_start]My research journey started later than usual due to my volunteer service experience[cite: 20]. [cite_start]However, I possess an unparalleled passion for exploration and scientific inquiry[cite: 21]. [cite_start]Over the past year of dedicated research study, I have solidified my aspiration to contribute to the field of Natural Language Processing (NLP)[cite: 22]. [cite_start]I am actively pursuing this goal, much like my unwavering commitment to morning runs over the past five years[cite: 22]. [cite_start]I firmly believe that "pain is inevitable, suffering is optional" and I am ready to embrace the challenges that come with pursuing what I desire[cite: 23]. [cite_start]Specifically, I have delved into research on natural language and multi-modal approaches, shaping me into an independent researcher with clear ideas about future research directions[cite: 24].

My research interests include:
* [cite_start]Scaling of Text/Multimodal Reasoning [cite: 24]
* [cite_start]Efficient Learning for Training and Inference [cite: 24]
* [cite_start]Domain Knowledge Injection [cite: 24]

---


# News

* [cite_start]**July 2025:** My paper, "[EMMA: An Enhanced MultiModal Reasoning Benchmark](YOUR_EMMA_ARXIV_LINK)"[cite: 6], has been accepted to ICML 2025 as an Oral presentation!
* [cite_start]**July 2025:** My paper, "[Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively](YOUR_SRM_ARXIV_LINK)"[cite: 9], has been accepted to ACL 2025 as an Oral Presentation!
* [cite_start]**July 2025:** My paper, "[Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](YOUR_CORE_ARXIV_LINK)"[cite: 11], has been accepted to ACL 2025!
* [cite_start]**July 2025:** My paper, "[MolRAG: Unlocking the Power of Large Language Models for Molecular Property Prediction](YOUR_MOLRAG_ARXIV_LINK)"[cite: 15], has been accepted to ACL 2025!
* [cite_start]**November 2024:** My paper, "[CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models](YOUR_CMR_ARXIV_LINK)"[cite: 4], has been accepted to EMNLP 2024 as a First Author paper and selected for Oral Presentation!

---

# Publications

1.  [cite_start]**[CMR Scaling Law: Predicting Critical Mixture Ratios for Continual Pre-training of Language Models](YOUR_PAPER_PDF_LINK)** [cite: 4]
    **Jiawei Gu***, et al.
    [cite_start]EMNLP 2024, **First Author**, Oral Presentation. [cite: 4]
    * [cite_start]**Contribution**: We attempt to re-visit the scaling behavior of LLMs under the hood of CPT, and introduce Critical Mixture Ratio (CMR) and a CMR scaling law, providing a predictive framework and practical guidelines for optimizing LLM training in specialized domains, ensuring both general and domain-specific performance while managing training resources effectively[cite: 5].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Slides](YOUR_SLIDES_LINK)

2.  **[Can MLLMs Reason/Think in Multimodality? [cite_start]EMMA: An Enhanced MultiModal Reasoning Benchmark](YOUR_PAPER_PDF_LINK)** [cite: 6]
    **Jiawei Gu***, et al.
    [cite_start]ICML 2025 (**Oral**), **First Author**. [cite: 6]
    * [cite_start]**Contribution**: This work introduces EMMA, a benchmark designed to evaluate multimodal reasoning across mathematics, physics, chemistry, and coding[cite: 7]. [cite_start]EMMA features tasks requiring advanced visual manipulation and cross-modal reasoning, providing insights into the limitations of current MLLMs and emphasizing the need for improved architectures and training paradigms[cite: 8].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Homepage](YOUR_PAPER_HOMEPAGE_LINK) &nbsp; [Slides](YOUR_SLIDES_LINK)

3.  [cite_start]**[Boosting Decision-Making Ability of LLMs with Speculative Reward Model Cost-Effectively](YOUR_PAPER_PDF_LINK)** [cite: 9]
    **Jiawei Gu***, et al.
    [cite_start]ACL 2025 (**Oral Presentation**), **First Author**. [cite: 9]
    * [cite_start]**Contribution**: A Plug and Play framework with Speculative Reward Models (SRM), which simplifies the complex process of achieving an optimal balance between effectiveness and efficiency[cite: 10].
    [PDF](YOUR_PAPER_PDF_LINK)

4.  [cite_start]**[Toward Structured Knowledge Reasoning: Contrastive Retrieval-Augmented Generation on Experience](YOUR_PAPER_PDF_LINK)** [cite: 11]
    **Jiawei Gu***, et al.
    [cite_start]ACL 2025, **First Author**. [cite: 11]
    * [cite_start]**Contribution**: A plug-and-play method, CoRE, is proposed to improve the reasoning ability on structured knowledge[cite: 12]. [cite_start]It is training-free, lifelong, and continuous[cite: 13].
    [PDF](YOUR_PAPER_PDF_LINK)

5.  [cite_start]**[A Survey on LLM-as-a-Judge](YOUR_PAPER_PDF_LINK)** [cite: 13]
    **Jiawei Gu***, et al.
    [cite_start]Preprint, **First Author**. [cite: 13]
    * [cite_start]**Contribution**: This survey provides a comprehensive review of strategies to enhance the reliability of LLM-as-a-Judge systems, introduces a benchmark for systematic evaluation, and discusses practical applications, challenges, and future directions to guide research and deployment[cite: 14].
    [PDF](YOUR_PAPER_PDF_LINK) &nbsp; [Homepage](YOUR_PAPER_HOMEPAGE_LINK)

6.  [cite_start]**[MolRAG: Unlocking the Power of Large Language Models for Molecular Property Prediction](YOUR_PAPER_PDF_LINK)** [cite: 15]
    Second Author.
    [cite_start]ACL 2025. [cite: 15]

7.  [cite_start]**[Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models](YOUR_PAPER_PDF_LINK)** [cite: 16]
    Second Author.
    [cite_start]Preprint. [cite: 16]

8.  [cite_start]**[Unfolding Spatial Cognition: Evaluating Multimodal Models on Visual Simulations](YOUR_PAPER_PDF_LINK)** [cite: 17]
    Third Author.
    [cite_start]Preprint. [cite: 17]
    [PDF](YOUR_PAPER_PDF_LINK)

9.  [cite_start]**[Full Front: Benchmarking MLLMs Across the Full Front-End Engineering Workflow](YOUR_PAPER_PDF_LINK)** [cite: 16]
    Third Author.
    [cite_start]Preprint. [cite: 16]
    [PDF](YOUR_PAPER_PDF_LINK)

---
 -->
